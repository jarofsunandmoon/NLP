<h1 id="OKMBW">论文标题：Multi-to-Single: Reducing Multimodal Dependency in Emotion Recognition Through Contrastive Learning（多对一：通过对比学习减少情绪识别中的多模态依赖）</h1>
主要内容： 本研究提出了一种新型的多到单（M2S）情感识别模型。该模型结合了对比学习和时空稀疏注意机制（STS），并通过多模态对比预测编码（M2M CPC）来融合不同模态的数据特征。模型通过在预训练阶段使用未标记的多模态数据进行特征学习，并在微调阶段只使用单一模态进行情绪识别。这种方法不仅能减少对多模态数据的依赖，还能在保持多模态性能的同时，提升单模态的识别准确性。  



<h3 id="HdXEg">背景介绍</h3>
 	多模态情感识别（Emotion Recognition）是情感脑机接口（BCI）领域的重要研究方向，旨在通过结合多种生理信号（如EEG、ECG、PPS和眼动信号）识别情绪状态。然而，在实际应用中，获得所有模态数据常常面临挑战，尤其是在信号质量不稳定或受限的环境中。因此，研究人员提出了一种跨模态方法，通过在训练阶段学习多个模态的联合表示，在测试阶段使用单一模态进行情绪识别，以降低对多模态数据的依赖。  

![image.png](https://raw.githubusercontent.com/jarofsunandmoon/img/1762529262393-ad3ae4de-b083-4f9c-b03c-03a53604df89.png)

![image.png](https://raw.githubusercontent.com/jarofsunandmoon/img/1762529262393-ad3ae4de-b083-4f9c-b03c-03a53604df89.png)

<h5 id="dXn0o">模型预训练阶段</h5>
模型的整体架构和预训练阶段如上图1(a)所示。每种模态设计了两个编码器。一种是基于变换的情绪相关(ER)编码器，用于从数据中提取与情绪有关的特征;另一种是基于mlp的情绪独立(EI)编码器，用于提取与情绪无关的特征。ER编码器包含几个由前馈和LayerNormalization操作组成的变压器块。对于EEG和其他一些信号，作者设计了一个特殊的STS注意。为了最小化两个编码器提取的特征之间的相关性，作者使用了对比对数比上界(CLUB)方法。与InfoNCE等传统方法不同，CLUB可以最小化互信息的上界，使两组数据尽可能独立。给定任意模态X，根据条件分布为未知的变分CLUB项，则X的CLUB损失定义为:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762529952461-04a9185c-1f01-4e20-a3f4-37056a9c2450.png)

其中z^X和z′^X分别为ER和EI编码器的输出。qθ(z′^X|z^X)是一个参数为θ的变分分布，近似于p(z′^X|z^X)。为了验证编码器是否提取了数据的有效特征，为每个模态添加了一个解码器，并计算每个模态的重建损失。模态X的重构损失定义为:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762530107347-914cab28-002d-476a-970e-18b2e95e84c5.png)

式中DX表示模态X的解码器。

为了学习和融合不同模式的特征，作者使用对比学习方法并提出了一种新的M2M CPC模块。对于任意嵌入向量z^X，通过时间窗维度的平均池化层和线性层将其投影到新的嵌入空间中。我们将模态X的最终嵌入定义为$ \bar{z}_X \in \mathbb{R}^{S \times D_f}
 $，其中S表示样本数，Df表示最终嵌入维数。其中有S对正样本对和S对2−S对负样本对。根据InfoNCE的思想，我们可以把它看作是一个s级分类任务。以EEG和EYE数据为例，我们可以定义地真值GT为[0, 1, . . . , S−1]计算对比损失LContra如下:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762532492595-4e9dd0c5-62ce-4162-8a26-dfc573dfd019.png)

<h5 id="YGCbr">时空稀疏注意机制（Spatial and Temporal-Sparse Attention Mechanism，STS）</h5>
在实验中使用了多种信号，包括EEG、心电和PPC。这些信号有以下共性:它们都是通过人体某些固定位置的电极长时间连续采集的。因此，这些信号在空间和时间维度上都具有很强的内在特征。作者提出了一个时空稀疏(STS)注意机制来充分利用这一特性。STS的结构如图1(c)所示。对于任意时间窗口内的数据，除了学习电极之间的结构信息外，还在时间维度上融合数据。经过线性变换后，取时间t、t−1和初始时间值的平均值为$ v_t^A
 $和$ k_t^A
 $。该操作基于以下考虑:首先，每个时间窗口内的初始数据确定了数据段的基本信息;其次，数据每一时刻都与前一时刻密切相关。所以现在有:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762530835298-2fadeb50-fb89-45d8-8a19-240cbeaddc52.png)

其中⊕代表求和和平均。给定注意力维度Datt，于是可以这样计算注意力:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762530911670-557adbd2-e5c2-4059-8f40-ee027018d147.png)

<h5 id="YaVIe">多对多对比预测编码（M2MCPC）</h5>
为了了解不同模式之间的特征，作者提出了一种新的多对多(M2M) CPC模块，如图2所示。传统的CPC模块通过自回归模型预测序列中未来嵌入向量与当前嵌入向量之间的相关性来提取数据特征。对于人类的情绪变化，往往会导致人体多个生理信号同时发生变化。在实际情况下，可以通过检测各种生理信号的变化来全面判断被试当前的情绪状态，这也是传统的多模态模型通常比单模态模型性能更好的重要原因。因此，在M2MCPC模块中，使用来自多个模态的当前嵌入向量来预测每个模态的未来向量。以EEG和EYE数据为例，通过它们的情绪相关编码器，得到嵌入向量:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762532027734-add9701d-1fe7-4658-9567-5eb1ecd86ffc.png)

式中，T为时间窗长度，S为数据样本数，D为嵌入维数。使用两层LSTM作为每个模态的自回归模型。定义M为的长度观察到的序列，N作为预测步长，有:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762532087420-2d45d020-2130-4c62-b398-8007c6173460.png)

其中$ D' $是LSTM隐藏层，并且$ M+N<=T
 $。然后将$ \hat{z}^{eeg}_M
 $和$ \hat{z}^{eye}_M $连接起来预测未来每个模态N步。使用InfoNCE loss对模块进行优化。损失函数由四部分组成:同一模态下预测向量和实向量的损失以及另一模态下预测向量和实向量的损失。定义$ Z_A $和$ Z_B $为一组负样本和一个正样本，其中A, B∈{eeg, eye}。则有:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762533192712-e55864e1-a0c1-4e6f-8d26-e0096a764dc9.png)

其中[·]表示连接操作。所以完整的M2M CPC损失为:

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762533243119-2c35ffd1-bf30-4360-a8df-0686e88006bb.png)

<h5 id="xtkNC">预训练和微调</h5>
考虑到以上所有的损失，可得到最后预训练阶段的最终损失：

![](https://cdn.nlark.com/yuque/0/2025/png/12829884/1762604627379-30eb5add-a0b2-46be-b88b-1091a4f8ef59.png)

其中α′，β′，γ′，λ′是各个损失项的权重系数。

在微调阶段，我们只需要输入一种模态的数据，冻结它的编码器和线性投影层，并添加一个分类器进行优化。对于跨模态任务，我们使用不同的模态进行测试。对于单模态任务，微调和测试过程类似于传统的监督学习。

<h3 id="S7uGp">实验</h3>
<h4 id="miTVU">数据集设计和实验细节</h4>
<h5 id="zcdiS">在SEED、SEED-IV和SEED-V数据集上</h5>
![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762604876140-79e15e20-77c2-4037-99f4-aba98995d32b.png)

![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762605419663-643dbca5-4043-4f3d-97dd-f5da00d1ee59.png)

对于训练集和测试集的划分，由于SEED系列数据集中每个视频片段对应的情感标签是固定的，将SEED、SEED-IV和SEED-V数据集分别按9:6、16:8和10:5的比例进行划分。DEAP和DREAMER数据集中的标签是受试者在某些评估指标上的分数，包括效价、唤醒和优势。这个**标签导致数据分布不均匀**，因此作者分别对DEAP和DREAMER进行了四重和三重交叉验证。每一叠的训练和测试比例为3:1和2:1分别表示。

作者选择平衡精度和kappa分数作为SEED系列数据集的分类评价指标。由DEAP和DREAMER数据集中的个体受试者在某一折叠中只有一个类别的标签，因此很难计算每个受试者的kappa分数。因此，作者计算了这两个数据集的平衡精度和F1分数。

<h5 id="nc9o5">Baselines</h5>
![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762606367206-ca3718c9-f799-4343-9fec-ee3b062dfefd.png)

![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762606397372-8bf46261-7299-4fe7-8355-d5828a62404a.png)

作者选择了几种最先进的跨模态方法作为基线，包括BADE-regressor，BADE-cGAN，ECO-FET，CLIP和统一码。使用与CLIP模型相同的编码器和微调过程，以确保比较的公平性。还比较了一些经典和最先进的监督多模态方法，包括BDAE，情感变压器融合(ETF)，VigilanceNet和MAET模型。以确定模型的预训练和对比学习模块是否在不同模态之间的关键信息学习和融合中发挥作用。

<h4 id="TNrzU">实验结果</h4>
SEED系列数据集的跨模态和单模态实验结果如表1所示，DEAP和DREAMER数据集的结果如表3所示。如表所示，模型在各种设置下的表现优于之前的所有工作。无论是跨模态任务还是单模态任务，准确率都有了显著提高。在所有跨模态任务中，与现有的次优方法相比，文中的模型一般提高了5个百分点以上，最大提高超过10个百分点(独立样本t检验，p < 0.05)。在使用某种模态进行微调后，单模态的测试性能略好于交叉模态。与多模态方法比较与监督多模态方法比较的结果如表4所示。由于EEG仍然是大多数研究中最受关注的数据，因此作者只给出了模型对EEG模态的微调结果。如表所示，只有一个双层MLP作为微调的分类器，M2S模型优于大多数有监督的多模态方法。同时，作者测试了仅使用EEG的ER编码器的单峰监督训练。其准确率略低于使用预训练模型，但仍能达到一些多模态方法的性能。实验结果表明，设置的预训练过程起到了一定的作用，对比学习模块的引入也使得来自不同特征的有效融合成为可能。

<h4 id="D5zVB">消融实验</h4>
![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762607166064-b4936522-af82-4208-b649-20f4257ae113.png)

![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762607183614-a51cddb8-0453-4177-90e8-fdebd6cd8413.png)

![](https://cdn.jsdelivr.net/gh/jarofsunandmoon/img/1762607203749-a0f8caab-d170-489d-937f-a12224394168.png)

**不同损失函数的影响**如表1所示，作者对模型中的四种损失函数分别进行了消融实验。去掉某个损失函数后，模型的性能会有不同程度的下降。去掉对比学习的损失函数对跨模态任务的影响**最为严重**。

**为了验证STS注意机**制的有效性，作者在SEED系列数据集上进行了以下实验：用常规多头注意机制替换STS注意机制或直接移除注意机制。由于STS注意机制仅适用于电极收集的信号，因此只给出EEG单峰微调的结果。如图3所示，使用STS注意机制后，模型的性能得到了显著提高。

**M2M CPC模块的消融实验**为了分析M2M CPC模块的影响，首先测试了加入该模块后现有方法的性能。结果如图4所示。加入M2M CPC后，各模态通过预测编码学习彼此的特征，实现特征融合，提高准确率。作者还测试了不同时间窗长度和预测步长对最终准确率的影响。作者在SEED数据集上绘制了两个跨模式任务的热图。如图5所示，在相同的超参数调优范围内，当时间窗长度固定时，随着预测步数的增加，模型的精度一般呈现先上升后下降的趋势。当预测步长固定时，更长的时间窗有助于模型的性能更好。

<h3 id="lKHU0">结论</h3>
经过在多个数据集上的实验验证，本文提出的跨模态学习方法M2S可有效减少情绪识别中的多模态依赖，并且用单模态实现了多模态性能。

